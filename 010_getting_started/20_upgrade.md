### [1.5 Upgrading From Previous Versions](#upgrade)<a id="upgrade"></a>

#### [Upgrading from 0.8.x or 0.9.x to 0.10.0.0](#upgrade_10)<a id="upgrade_10"></a>

0.10.0.0 has [**potential breaking changes**](http://kafka.apache.org/documentation.html#upgrade_10_breaking) \(please review before upgrading\) and possible [**performance impact following the upgrade**](http://kafka.apache.org/documentation.html#upgrade_10_performance_impact). By following the recommended rolling upgrade plan below, you guarantee no downtime and no performance impact during and following the upgrade. 

Note: Because new protocols are introduced, it is important to upgrade your Kafka clusters before upgrading your clients.

**Notes to clients with version 0.9.0.0: **Due to a bug introduced in 0.9.0.0, clients that depend on ZooKeeper \(old Scala high-level Consumer and MirrorMaker if used with the old consumer\) will not work with 0.10.0.x brokers. Therefore, 0.9.0.0 clients should be upgraded to 0.9.0.1 **before** brokers are upgraded to 0.10.0.x. This step is not necessary for 0.8.X or 0.9.0.1 clients.

**For a rolling upgrade:**

1. Update server.properties file on all brokers and add the following properties:
  * inter.broker.protocol.version=CURRENT\_KAFKA\_VERSION \(e.g. 0.8.2 or 0.9.0.0\).
  * log.message.format.version=CURRENT\_KAFKA\_VERSION \(See [**potential performance impact following the upgrade**](http://kafka.apache.org/documentation.html#upgrade_10_performance_impact) for the details on what this configuration does.\)

2. Upgrade the brokers. This can be done a broker at a time by simply bringing it down, updating the code, and restarting it.
3. Once the entire cluster is upgraded, bump the protocol version by editing inter.broker.protocol.version and setting it to 0.10.0.0. NOTE: You shouldn't touch log.message.format.version yet - this parameter should only change once all consumers have been upgraded to 0.10.0.0
4. Restart the brokers one by one for the new protocol version to take effect.
5. Once all consumers have been upgraded to 0.10.0, change log.message.format.version to 0.10.0 on each broker and restart them one by one.

**Note:** If you are willing to accept downtime, you can simply take all the brokers down, update the code and start all of them. They will start with the new protocol by default.

**Note:** Bumping the protocol version and restarting can be done any time after the brokers were upgraded. It does not have to be immediately after.

##### [Potential performance impact following upgrade to 0.10.0.0](#upgrade_10_performance_impact)<a id="upgrade_10_performance_impact"></a>

The message format in 0.10.0 includes a new timestamp field and uses relative offsets for compressed messages. The on disk message format can be configured through log.message.format.version in the server.properties file. The default on-disk message format is 0.10.0. If a consumer client is on a version before 0.10.0.0, it only understands message formats before 0.10.0. In this case, the broker is able to convert messages from the 0.10.0 format to an earlier format before sending the response to the consumer on an older version. However, the broker can't use zero-copy transfer in this case. Reports from the Kafka community on the performance impact have shown CPU utilization going from 20% before to 100% after an upgrade, which forced an immediate upgrade of all clients to bring performance back to normal. To avoid such message conversion before consumers are upgraded to 0.10.0.0, one can set log.message.format.version to 0.8.2 or 0.9.0 when upgrading the broker to 0.10.0.0. This way, the broker can still use zero-copy transfer to send the data to the old consumers. Once consumers are upgraded, one can change the message format to 0.10.0 on the broker and enjoy the new message format that includes new timestamp and improved compression. The conversion is supported to ensure compatibility and can be useful to support a few apps that have not updated to newer clients yet, but is impractical to support all consumer traffic on even an overprovisioned cluster. Therefore it is critical to avoid the message conversion as much as possible when brokers have been upgraded but the majority of clients have not.

For clients that are upgraded to 0.10.0.0, there is no performance impact.

**Note:** By setting the message format version, one certifies that all existing messages are on or below that message format version. Otherwise consumers before 0.10.0.0 might break. In particular, after the message format is set to 0.10.0, one should not change it back to an earlier format as it may break consumers on versions before 0.10.0.0.

**Note:** Due to the additional timestamp introduced in each message, producers sending small messages may see a message throughput degradation because of the increased overhead. Likewise, replication now transmits an additional 8 bytes per message. If you're running close to the network capacity of your cluster, it's possible that you'll overwhelm the network cards and see failures and performance issues due to the overload.

**Note:** If you have enabled compression on producers, you may notice reduced producer throughput and\/or lower compression rate on the broker in some cases. When receiving compressed messages, 0.10.0 brokers avoid recompressing the messages, which in general reduces the latency and improves the throughput. In certain cases, however, this may reduce the batching size on the producer, which could lead to worse throughput. If this happens, users can tune linger.ms and batch.size of the producer for better throughput. In addition, the producer buffer used for compressing messages with snappy is smaller than the one used by the broker, which may have a negative impact on the compression ratio for the messages on disk. We intend to make this configurable in a future Kafka release.



##### [Potential breaking changes in 0.10.0.0](#upgrade_10_breaking)<a id="upgrade_10_breaking"></a>

* Starting from Kafka 0.10.0.0, the message format version in Kafka is represented as the Kafka version. For example, message format 0.9.0 refers to the highest message version supported by Kafka 0.9.0.
* Message format 0.10.0 has been introduced and it is used by default. It includes a timestamp field in the messages and relative offsets are used for compressed messages.
* ProduceRequest\/Response v2 has been introduced and it is used by default to support message format 0.10.0
* FetchRequest\/Response v2 has been introduced and it is used by default to support message format 0.10.0
* MessageFormatter interface was changed from `def writeTo(key: Array[Byte], value: Array[Byte], output: PrintStream)` to `def writeTo(consumerRecord: ConsumerRecord[Array[Byte], Array[Byte]], output: PrintStream)`
* MessageReader interface was changed from `def readMessage(): KeyedMessage[Array[Byte], Array[Byte]]` to `def readMessage(): ProducerRecord[Array[Byte], Array[Byte]]`
* MessageFormatter's package was changed from `kafka.tools` to `kafka.common`
* MessageReader's package was changed from `kafka.tools` to `kafka.common`
* MirrorMakerMessageHandler no longer exposes the `handle(record: MessageAndMetadata[Array[Byte], Array[Byte]])` method as it was never called.
* The 0.7 KafkaMigrationTool is no longer packaged with Kafka. If you need to migrate from 0.7 to 0.10.0, please migrate to 0.8 first and then follow the documented upgrade process to upgrade from 0.8 to 0.10.0.
* The new consumer has standardized its APIs to accept `java.util.Collection` as the sequence type for method parameters. Existing code may have to be updated to work with the 0.10.0 client library.
* LZ4-compressed message handling was changed to use an interoperable framing specification \(LZ4f v1.5.1\). To maintain compatibility with old clients, this change only applies to Message format 0.10.0 and later. Clients that Produce\/Fetch LZ4-compressed messages using v0\/v1 \(Message format 0.9.0\) should continue to use the 0.9.0 framing implementation. Clients that use Produce\/Fetch protocols v2 or later should use interoperable LZ4f framing. A list of interoperable LZ4 libraries is available at http:\/\/www.lz4.org\/

##### [Notable changes in 0.10.0.0](#upgrade_10_notable)<a id="upgrade_10_notable"></a>

* Starting from Kafka 0.10.0.0, a new client library named **Kafka Streams** is available for stream processing on data stored in Kafka topics. This new client library only works with 0.10.x and upward versioned brokers due to message format changes mentioned above. For more information please read [**this section**](http://kafka.apache.org/documentation.html#streams_overview).
* The default value of the configuration parameter `receive.buffer.bytes` is now 64K for the new consumer.
* The new consumer now exposes the configuration parameter `exclude.internal.topics` to restrict internal topics \(such as the consumer offsets topic\) from accidentally being included in regular expression subscriptions. By default, it is enabled.
* The old Scala producer has been deprecated. Users should migrate their code to the Java producer included in the kafka-clients JAR as soon as possible.
* The new consumer API has been marked stable.

#### [Upgrading from 0.8.0, 0.8.1.X or 0.8.2.X to 0.9.0.0](#upgrade_9)<a id="upgrade_9"></a>

0.9.0.0 has [**potential breaking changes**](http://kafka.apache.org/documentation.html#upgrade_9_breaking) \(please review before upgrading\) and an inter-broker protocol change from previous versions. This means that upgraded brokers and clients may not be compatible with older versions. It is important that you upgrade your Kafka cluster before upgrading your clients. If you are using MirrorMaker downstream clusters should be upgraded first as well.

**For a rolling upgrade:**

1. Update server.properties file on all brokers and add the following property: inter.broker.protocol.version=0.8.2.X
2. Upgrade the brokers. This can be done a broker at a time by simply bringing it down, updating the code, and restarting it.
3. Once the entire cluster is upgraded, bump the protocol version by editing inter.broker.protocol.version and setting it to 0.9.0.0.
4. Restart the brokers one by one for the new protocol version to take effect

**Note:** If you are willing to accept downtime, you can simply take all the brokers down, update the code and start all of them. They will start with the new protocol by default.

**Note:** Bumping the protocol version and restarting can be done any time after the brokers were upgraded. It does not have to be immediately after.

##### [Potential breaking changes in 0.9.0.0](#upgrade_9_breaking)<a id="upgrade_9_breaking"></a>

* Java 1.6 is no longer supported.
* Scala 2.9 is no longer supported.
* Broker IDs above 1000 are now reserved by default to automatically assigned broker IDs. If your cluster has existing broker IDs above that threshold make sure to increase the reserved.broker.max.id broker configuration property accordingly.
* Configuration parameter replica.lag.max.messages was removed. Partition leaders will no longer consider the number of lagging messages when deciding which replicas are in sync.
* Configuration parameter replica.lag.time.max.ms now refers not just to the time passed since last fetch request from replica, but also to time since the replica last caught up. Replicas that are still fetching messages from leaders but did not catch up to the latest messages in replica.lag.time.max.ms will be considered out of sync.
* Compacted topics no longer accept messages without key and an exception is thrown by the producer if this is attempted. In 0.8.x, a message without key would cause the log compaction thread to subsequently complain and quit \(and stop compacting all compacted topics\).
* MirrorMaker no longer supports multiple target clusters. As a result it will only accept a single --consumer.config parameter. To mirror multiple source clusters, you will need at least one MirrorMaker instance per source cluster, each with its own consumer configuration.
* Tools packaged under _org.apache.kafka.clients.tools.\*_ have been moved to _org.apache.kafka.tools.\*_. All included scripts will still function as usual, only custom code directly importing these classes will be affected.
* The default Kafka JVM performance options \(KAFKA\_JVM\_PERFORMANCE\_OPTS\) have been changed in kafka-run-class.sh.
* The kafka-topics.sh script \(kafka.admin.TopicCommand\) now exits with non-zero exit code on failure.
* The kafka-topics.sh script \(kafka.admin.TopicCommand\) will now print a warning when topic names risk metric collisions due to the use of a '.' or '\_' in the topic name, and error in the case of an actual collision.
* The kafka-console-producer.sh script \(kafka.tools.ConsoleProducer\) will use the new producer instead of the old producer be default, and users have to specify 'old-producer' to use the old producer.
* By default all command line tools will print all logging messages to stderr instead of stdout.

##### [Notable changes in 0.9.0.1](#upgrade_901_notable)<a id="upgrade_901_notable"></a>

* The new broker id generation feature can be disabled by setting broker.id.generation.enable to false.
* Configuration parameter log.cleaner.enable is now true by default. This means topics with a cleanup.policy=compact will now be compacted by default, and 128 MB of heap will be allocated to the cleaner process via log.cleaner.dedupe.buffer.size. You may want to review log.cleaner.dedupe.buffer.size and the other log.cleaner configuration values based on your usage of compacted topics.
* Default value of configuration parameter fetch.min.bytes for the new consumer is now 1 by default.

##### Deprecations in 0.9.0.0

* Altering topic configuration from the kafka-topics.sh script \(kafka.admin.TopicCommand\) has been deprecated. Going forward, please use the kafka-configs.sh script \(kafka.admin.ConfigCommand\) for this functionality.
* The kafka-consumer-offset-checker.sh \(kafka.tools.ConsumerOffsetChecker\) has been deprecated. Going forward, please use kafka-consumer-groups.sh \(kafka.admin.ConsumerGroupCommand\) for this functionality.
* The kafka.tools.ProducerPerformance class has been deprecated. Going forward, please use org.apache.kafka.tools.ProducerPerformance for this functionality \(kafka-producer-perf-test.sh will also be changed to use the new class\).
* The producer config block.on.buffer.full has been deprecated and will be removed in future release. Currently its default value has been changed to false. The KafkaProducer will no longer throw BufferExhaustedException but instead will use max.block.ms value to block, after which it will throw a TimeoutException. If block.on.buffer.full property is set to true explicitly, it will set the max.block.ms to Long.MAX\_VALUE and metadata.fetch.timeout.ms will not be honoured

#### [Upgrading from 0.8.1 to 0.8.2](#upgrade_82)<a id="upgrade_82"></a>

0.8.2 is fully compatible with 0.8.1. The upgrade can be done one broker at a time by simply bringing it down, updating the code, and restarting it.

#### [Upgrading from 0.8.0 to 0.8.1](#upgrade_81)<a id="upgrade_81"></a>

0.8.1 is fully compatible with 0.8. The upgrade can be done one broker at a time by simply bringing it down, updating the code, and restarting it.

#### [Upgrading from 0.7](#upgrade_7)<a id="upgrade_7"></a>

Release 0.7 is incompatible with newer releases. Major changes were made to the API, ZooKeeper data structures, and protocol, and configuration in order to add replication \(Which was missing in 0.7\). The upgrade from 0.7 to later versions requires a [**special tool**](https://cwiki.apache.org/confluence/display/KAFKA/Migrating+from+0.7+to+0.8) for migration. This migration can be done without downtime.
